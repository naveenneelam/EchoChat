# <img width="50" height="50" alt="ChatGPT Image Dec 11, 2025, 08_17_15 PM" src="https://github.com/user-attachments/assets/9dd5a0a4-82a8-41ae-b38b-66db74e608fe" />
**EchoChat**

**EchoChat** is a fully containerized, real-time **voice-driven note-taking and AI interaction system**.
It uses **ReactJS** (frontend), **Python FastAPI** (backend), **WebSockets** for live audio streaming, **ONNX Parakeet ASR** for speech-to-text, **Silero VAD** for voice activity detection, and **Ollama-powered local LLMs** for intent detection and action execution.

EchoChat allows users to capture audio through the microphone, view live multi-color waveform activity, generate accurate speech transcription, detect user intentions from the transcribed text, and trigger contextual actionsâ€”such as creating categorized notesâ€”based on confirmed commands.

---

## **âœ¨ Key Features**

### ğŸ™ï¸ **Real-Time Voice Input**

* Capture audio directly from the browser microphone.
* Live voice segmentation using **Silero VAD**.
* Multi-color waveform visualization (silence / speech / processing / confirmation).

### ğŸ“ **Accurate Speech Transcription**

* CPU-optimized **ONNX Parakeet ASR** model.
* Streaming transcription via WebSocket.
* Automatic punctuation and formatting.

### ğŸ¤– **Local LLM-driven Action Engine**

* Powered by **Ollama** running local LLMs.
* Extracts:

    * Intent
    * Action
    * Category
    * Additional parameters (e.g., note contents)

### âœ”ï¸ **User-Confirmed Actions**

* Actions run **only when the user says a confirmation keyword**, e.g.:

    * **"confirm and submit"**
    * **"thats all"**
* Example:
  â€œCreate a note in electronics category about Arduino sensorsâ€¦ **confirm and submit**â€

### ğŸ”Œ **WebSocket Architecture**

* Low-latency interaction between React frontend and Python backend.
* Streams audio â†’ processes â†’ returns partial + final transcripts.
### ğŸ”Œ **ScreenShots**
<img width="720" height="624" alt="image" src="https://github.com/user-attachments/assets/f048cd70-90a9-46a1-be9c-0e35a98c4b19" />
<img width="720" height="618" alt="image" src="https://github.com/user-attachments/assets/af8b3750-1d91-450c-9d71-6151e7acb2b8" />


### ğŸ³ **Docker Compose Deployment**

* Fully isolated multi-container environment.
* Services:

    * `frontend` (React)
    * `backend` (FastAPI)

---

## **ğŸ“ Project Structure**

```
EchoChat/
â”‚
â”œâ”€â”€ frontend/              # ReactJS UI
â”‚   â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ dist/
â”‚   â”œâ”€â”€ node_modules/
â”‚   â”œâ”€â”€ index.tsx
â”‚   â”œâ”€â”€ index.html/
â”‚   â”œâ”€â”€ .....
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ backend/               # Python FastAPI backend
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
|
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

---

## **ğŸš€ Getting Started**

### **1ï¸âƒ£ Prerequisites**

* Docker & Docker Compose installed
* 4GB+ RAM recommended (for LLMs)
* Microphone permissions enabled in browser

---

## **2ï¸âƒ£ Clone the Repository**

```bash
git clone https://github.com/<your-username>/EchoChat.git
cd EchoChat
```

---

## **3ï¸âƒ£ Start the Stack**

```bash
docker compose up --build
```

---

## **4ï¸âƒ£ Access the App**

* Frontend: **[http://localhost:81]**
* Ollama server (external + local): **[http://localhost:11434](http://localhost:11434)**

---

## **ğŸ§  AI Components**

### **Speech-to-Text**

* Model: **ONNX Parakeet (CPU optimized)**
* Streaming inference
* Low latency even on low-end machines

### **Voice Activity Detection**

* **Silero VAD** identifies:

    * Speech vs silence
    * Threshold-based segmentation
    * Color-coded waveform display

### **LLM Intent Classification**

Using Ollama models (e.g., llama3, mistral, qwen), the system extracts:

| Field                    | Description                                               |
| ------------------------ | --------------------------------------------------------- |
| **Intent**               | What the user wants (create_note, search, reminder, etc.) |
| **Category**             | e.g., electronics, personal, work                         |
| **Content**              | Actual note or command details                            |
| **Action Required?**     | Yes/No based on keywords                                  |
| **Confirmation Needed?** | Triggered when user says â€œconfirm/submitâ€                 |

---

## **ğŸ§© How It Works (Flow)**

### **1. User speaks**

â¬‡

### **2. Browser captures audio â†’ streams via WebSocket**

â¬‡

### **3. Backend performs VAD + ASR transcription**

â¬‡

### **4. LLM analyzes transcript**

* Extracts intent
* Prepares actions
  â¬‡

### **5. Action executed only after confirmation**

â¬‡

### **6. Result returned to frontend**

---

## **ğŸ§ª Example User Scenario**

**User:**
â€œMake a note about Arduino sensors under electronics. I need this for tomorrow.
â€¦ confirmâ€

**Detected by system:**

* Intent: `create_note`
* Category: `electronics`
* Content: "Arduino sensorsâ€¦ tomorrow"
* Action: save note
* Status: executed after â€œconfirmâ€

---

## **ğŸ“¦ Docker Compose Overview**

```yaml
services:
  backend:
    build: ./backend
    ports:
      - "8765:8765"   
    environment:
      - stt-network
    networks:
      - stt-network
    volumes:
      - ./output_files:/app/output_files
      - ./notes:/app/notes      
  frontend:
    build: ./frontend
    ports:
      - "81:80"
    depends_on:
      - backend
    networks:
      - stt-network

networks:
  stt-network:
    driver: bridge

```

---

## **ğŸ‘¨â€ğŸ’» Development Mode**

Start frontend:

```bash
cd frontend
npm install
npm start
```

Start backend:

```bash
cd backend
pip install -r requirements.txt
python app.py
```

---

## **ğŸ“œ Roadmap**

* [ ] Embed-based memory for persistent notes
* [ ] Multi-speaker segmentation
* [ ] Offline mode (service worker)
* [ ] Mobile PWA
* [ ] Add whisper.cpp as optional ASR backend

---

## **ğŸ›¡ï¸ License**

MIT License Â© 2025

---
